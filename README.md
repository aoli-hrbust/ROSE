The demo code of Curriculum learning: Efficient Multi-view Representations for Incomplete Multi-view Clustering with Pseudo-supervised.

< img src="./docs/rose_framework.jpg" alt="Overview" width="80%">

An explanation of the pseudo-supervised module is provided here. By constraining the view-specific soft clustering assignments  with a global pseudo-label, this module encourages the resulting unified clustering assignment to preserve discriminative characteristics unique to each view while incorporating cross-view consistency information.

## Abstract

The rapid advancement of multi-view representation learning has spurred breakthroughs in incomplete multi-view clustering (IMC). Despite progress, prevailing multi-view representation learning methods often apply a uniform strategy to capture both consistent and view-specific information simultaneously, overlooking the inherent tendency of neural networks to prioritize cross-view consensus before gradually memorizing view-specific details. This mismatch leads to suboptimal representation quality in IMC tasks. To overcome this, we propose ROSE, an efficient multi-view representation framework that leverages a curriculum learning strategy to first guide the model toward learning cross-view consistent representations and then progressively shift focus to view-specific features, aligning the learning process with the natural behavior of neural networks. Moreover, each view is equipped with a learnable prior, enabling view-specific representations to adapt to their optimal distributions rather than being constrained by a predefined structure. Additionally, a pseudo-supervised clustering module integrates global pseudo-labels to align view-specific soft assignments, reducing cross-view discrepancies and enabling end-to-end clustering without post-processing. Extensive experiments on multiple multi-view datasets demonstrate that ROSE consistently outperforms baseline IMC methods across various metrics and missing rates.
